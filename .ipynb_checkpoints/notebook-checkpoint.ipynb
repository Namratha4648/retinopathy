{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dc7270-b2ba-4860-8d0a-d28ff273da55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id_code  diagnosis\n",
      "0  000c1434d8d7          2\n",
      "1  001639a390f0          4\n",
      "2  0024cdab0c1e          1\n",
      "3  002c21358ce6          0\n",
      "4  005b95c28852          0\n",
      "        id_code\n",
      "0  0005cfc8afb6\n",
      "1  003f0afdcd15\n",
      "2  006efc72b638\n",
      "3  00836aaacf06\n",
      "4  009245722fa4\n",
      "        id_code  diagnosis\n",
      "0  0005cfc8afb6          0\n",
      "1  003f0afdcd15          0\n",
      "2  006efc72b638          0\n",
      "3  00836aaacf06          0\n",
      "4  009245722fa4          0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tables\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df  = pd.read_csv('data/test.csv')\n",
    "sample_sub = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# Take a quick peek\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(sample_sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f30bd0-8dd5-4e9b-8740-6256940de3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000c1434d8d7.png', '001639a390f0.png', '0024cdab0c1e.png', '002c21358ce6.png', '005b95c28852.png', '0083ee8054ee.png', '0097f532ac9f.png', '00a8624548a9.png', '00b74780d31d.png', '00cb6555d108.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the first 10 files in data/train/\n",
    "files = os.listdir('data/train_images')\n",
    "print(files[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd6e778-984b-4b3a-a2d4-04e1eb539dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>data/train_images/000c1434d8d7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>data/train_images/001639a390f0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>data/train_images/0024cdab0c1e.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train_images/002c21358ce6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train_images/005b95c28852.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis                            filepath\n",
       "0  000c1434d8d7          2  data/train_images/000c1434d8d7.png\n",
       "1  001639a390f0          4  data/train_images/001639a390f0.png\n",
       "2  0024cdab0c1e          1  data/train_images/0024cdab0c1e.png\n",
       "3  002c21358ce6          0  data/train_images/002c21358ce6.png\n",
       "4  005b95c28852          0  data/train_images/005b95c28852.png"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming train_df and test_df are already loaded\n",
    "\n",
    "# Create a filepath column for PNG images\n",
    "train_df['filepath'] = train_df['id_code'].apply(lambda x: f\"data/train_images/{x}.png\")\n",
    "test_df ['filepath'] = test_df ['id_code'].apply(lambda x: f\"data/test_images/{x}.png\")\n",
    "\n",
    "# View the result\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59334539-812b-486f-8740-489ddfce4dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing training images: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Find any missing files\n",
    "missing = train_df.loc[~train_df['filepath'].apply(os.path.exists), 'filepath']\n",
    "print(\"Missing training images:\", len(missing))\n",
    "if len(missing) > 0:\n",
    "    print(missing.tolist()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac7ae45-46ff-45ec-b28c-7eb5ddf16b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49aae3e0-50da-4684-bb17-ff4271a7ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        img = Image.open(row['filepath']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5d148e-ef0a-4d3e-abcf-7a05ce9518fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7703d443-ceee-4fe5-a843-de8d107de435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 80/20 split\n",
    "train_size = int(0.8 * len(train_df))\n",
    "val_size   = len(train_df) - train_size\n",
    "train_df_subset, val_df_subset = random_split(train_df, [train_size, val_size], \n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Build Datasets\n",
    "train_ds = RetinopathyDataset(train_df_subset.dataset.loc[train_df_subset.indices], transform=data_transforms)\n",
    "val_ds   = RetinopathyDataset(val_df_subset.dataset.loc[val_df_subset.indices], transform=data_transforms)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49372b0-6574-4c04-a23b-f3a3ec5b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),       # make every image 224×224 pixels\n",
    "    transforms.ToTensor(),               # convert PIL → Tensor (0.0–1.0 floats)\n",
    "    transforms.Normalize(                # normalize to mean/std based on ImageNet\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0896f961-6bd5-47c5-a507-7be0ed19424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 80/20 train/validation split\n",
    "train_size = int(0.8 * len(train_df))\n",
    "val_size   = len(train_df) - train_size\n",
    "train_subset, val_subset = random_split(train_df, [train_size, val_size],  \n",
    "                                        generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Wrap them in our RetinopathyDataset\n",
    "train_ds = RetinopathyDataset(train_subset.dataset.loc[train_subset.indices], transform=data_transforms)\n",
    "val_ds   = RetinopathyDataset(val_subset.dataset.loc[val_subset.indices], transform=data_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dbec49f-0c34-48fe-8296-512a40a578b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: torch.Size([16, 3, 224, 224])\n",
      "Labels batch shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(\"Images batch shape:\", images.shape)   # Expected: [16, 3, 224, 224]\n",
    "print(\"Labels batch shape:\", labels.shape)   # Expected: [16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e882b87-27f5-41ad-8ed9-a378b6ce7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Load pretrained ResNet50 using the updated method\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Freeze earlier layers (optional, for faster training)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final FC layer for 5-class output\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 5)  # 5 DR levels (0 to 4)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfa4601-22d6-4c48-af5b-b60bfc972c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# # Load pre-trained ResNet50\n",
    "# model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# # ✅ Replace final layer to match your 5-class classification\n",
    "# num_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_features, 5)\n",
    "\n",
    "# # ✅ Move to device\n",
    "# model = model.to(device)\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pretrained model\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# ✅ Replace FC layer AFTER loading model\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 5)\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22a15470-01bb-4aae-94fd-5376501d96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                     classes=np.unique(train_df['diagnosis']),\n",
    "                                     y=train_df['diagnosis'])\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# ✅ Train full model, not just fc\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import numpy as np\n",
    "\n",
    "# class_weights = compute_class_weight('balanced', \n",
    "#                                      classes=np.unique(train_df['diagnosis']),\n",
    "#                                      y=train_df['diagnosis'])\n",
    "# weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.05)\n",
    "\n",
    "\n",
    "\n",
    "# # ✅ Train full model, not just fc\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc78c3fc-9ea2-4f29-a5e4-07676f4b1bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 1.6018\n",
      "Batch 10, Loss: 1.5720\n",
      "Batch 20, Loss: 1.4387\n",
      "Batch 30, Loss: 1.2935\n",
      "Batch 40, Loss: 1.2265\n",
      "Batch 50, Loss: 1.2756\n",
      "Batch 60, Loss: 1.1150\n",
      "Batch 70, Loss: 1.1095\n",
      "Batch 80, Loss: 0.5761\n",
      "Batch 90, Loss: 0.9800\n",
      "Batch 100, Loss: 0.9660\n",
      "Batch 110, Loss: 0.9218\n",
      "Batch 120, Loss: 1.3308\n",
      "Batch 130, Loss: 0.9931\n",
      "Batch 140, Loss: 0.9500\n",
      "Batch 150, Loss: 0.7825\n",
      "Batch 160, Loss: 0.8878\n",
      "Batch 170, Loss: 0.9494\n",
      "Batch 180, Loss: 0.6475\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd144ac3-3f52-4719-bacb-74ceb4aabbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 70.53%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "466cae74-03e1-4343-94ff-0923e90b994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f6bdf6-9ca2-4725-b4cf-b2b27f4d8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce92363f-d21d-40e6-a26e-ab6808d80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9430b4b-e597-4fbd-842b-603e8b564aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.4706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()          # ✅ this happens before scheduler.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # ✅ Step LR after the entire epoch is complete\n",
    "    scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e492aa-f1b7-4770-88b4-2f9e7936ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training loop code...\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1012b4-a412-43b3-a168-7ee2d3fff56c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
