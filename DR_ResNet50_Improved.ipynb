{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db3ef3c-d8f9-4f15-933f-4657cef38cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481e0a72-f98f-45e0-a2e5-43c17a6526a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision scikit-learn matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d789b3-dba9-4d8c-adb3-d89fe6c9bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78befa48-60b7-45b3-9f15-9756a6a382f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>data/train_images/000c1434d8d7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>data/train_images/001639a390f0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>data/train_images/0024cdab0c1e.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train_images/002c21358ce6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train_images/005b95c28852.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis                            filepath\n",
       "0  000c1434d8d7          2  data/train_images/000c1434d8d7.png\n",
       "1  001639a390f0          4  data/train_images/001639a390f0.png\n",
       "2  0024cdab0c1e          1  data/train_images/0024cdab0c1e.png\n",
       "3  002c21358ce6          0  data/train_images/002c21358ce6.png\n",
       "4  005b95c28852          0  data/train_images/005b95c28852.png"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_df['filepath'] = train_df['id_code'].apply(lambda x: f\"data/train_images/{x}.png\")\n",
    "test_df['filepath'] = test_df['id_code'].apply(lambda x: f\"data/test_images/{x}.png\")\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f589589-40cd-4321-8821-ea46801eb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_df['diagnosis'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2537a5c8-25cb-41e3-800d-25460ffb9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std= [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84bb34f-b937-40f3-a274-6e228d2617f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        img = Image.open(row['filepath']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7906c3-32fb-4f44-ac38-046a467faeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_ds = RetinopathyDataset(train_df_split, transform=data_transforms)\n",
    "val_ds = RetinopathyDataset(val_df_split, transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d9e5aa-dc9d-4bee-a982-7e163587e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 5)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba62095-24de-4e8a-b73d-8a5006b224ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced',\n",
    "                                     classes=np.unique(train_df['diagnosis']),\n",
    "                                     y=train_df['diagnosis'])\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c98d1c6d-7ab7-47ab-a205-2d48eec84fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"layer2\" in name or \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c9d53e-ccdb-45c0-a3c4-8359998f14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "419d3155-fb1d-4a6a-bcc9-f4d77c0e17c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 1.6903\n",
      "Epoch 0, Batch 10, Loss: 1.7192\n",
      "Epoch 0, Batch 20, Loss: 1.7261\n",
      "Epoch 0, Batch 30, Loss: 1.5960\n",
      "Epoch 0, Batch 40, Loss: 1.6742\n",
      "Epoch 0, Batch 50, Loss: 1.3711\n",
      "Epoch 0, Batch 60, Loss: 1.5401\n",
      "Epoch 0, Batch 70, Loss: 1.4416\n",
      "Epoch 0, Batch 80, Loss: 1.7228\n",
      "Epoch 0, Batch 90, Loss: 1.3808\n",
      "Epoch 0, Batch 100, Loss: 1.4534\n",
      "Epoch 0, Batch 110, Loss: 1.3839\n",
      "Epoch 0, Batch 120, Loss: 1.4745\n",
      "Epoch 0, Batch 130, Loss: 1.4609\n",
      "Epoch 0, Batch 140, Loss: 1.3181\n",
      "Epoch 0, Batch 150, Loss: 1.3907\n",
      "Epoch 0, Batch 160, Loss: 1.4253\n",
      "Epoch 0, Batch 170, Loss: 1.4937\n",
      "Epoch 0, Batch 180, Loss: 1.2909\n",
      "Epoch 1, Batch 0, Loss: 1.4724\n",
      "Epoch 1, Batch 10, Loss: 1.3333\n",
      "Epoch 1, Batch 20, Loss: 1.6228\n",
      "Epoch 1, Batch 30, Loss: 1.1856\n",
      "Epoch 1, Batch 40, Loss: 2.0418\n",
      "Epoch 1, Batch 50, Loss: 1.1089\n",
      "Epoch 1, Batch 60, Loss: 1.2280\n",
      "Epoch 1, Batch 70, Loss: 1.2579\n",
      "Epoch 1, Batch 80, Loss: 1.5039\n",
      "Epoch 1, Batch 90, Loss: 1.5523\n",
      "Epoch 1, Batch 100, Loss: 1.3345\n",
      "Epoch 1, Batch 110, Loss: 1.4999\n",
      "Epoch 1, Batch 120, Loss: 1.3123\n",
      "Epoch 1, Batch 130, Loss: 1.3638\n",
      "Epoch 1, Batch 140, Loss: 1.3517\n",
      "Epoch 1, Batch 150, Loss: 1.5124\n",
      "Epoch 1, Batch 160, Loss: 1.0004\n",
      "Epoch 1, Batch 170, Loss: 1.3403\n",
      "Epoch 1, Batch 180, Loss: 1.2129\n",
      "Epoch 2, Batch 0, Loss: 1.1524\n",
      "Epoch 2, Batch 10, Loss: 1.4170\n",
      "Epoch 2, Batch 20, Loss: 1.0383\n",
      "Epoch 2, Batch 30, Loss: 1.1620\n",
      "Epoch 2, Batch 40, Loss: 1.1639\n",
      "Epoch 2, Batch 50, Loss: 1.0714\n",
      "Epoch 2, Batch 60, Loss: 1.1489\n",
      "Epoch 2, Batch 70, Loss: 1.0665\n",
      "Epoch 2, Batch 80, Loss: 1.2011\n",
      "Epoch 2, Batch 90, Loss: 1.0651\n",
      "Epoch 2, Batch 100, Loss: 1.5347\n",
      "Epoch 2, Batch 110, Loss: 1.2100\n",
      "Epoch 2, Batch 120, Loss: 1.3444\n",
      "Epoch 2, Batch 130, Loss: 1.1963\n",
      "Epoch 2, Batch 140, Loss: 0.9761\n",
      "Epoch 2, Batch 150, Loss: 1.0999\n",
      "Epoch 2, Batch 160, Loss: 0.7292\n",
      "Epoch 2, Batch 170, Loss: 1.1034\n",
      "Epoch 2, Batch 180, Loss: 0.9669\n",
      "Epoch 3, Batch 0, Loss: 1.4561\n",
      "Epoch 3, Batch 10, Loss: 1.0519\n",
      "Epoch 3, Batch 20, Loss: 1.2487\n",
      "Epoch 3, Batch 30, Loss: 0.7466\n",
      "Epoch 3, Batch 40, Loss: 1.3136\n",
      "Epoch 3, Batch 50, Loss: 1.1263\n",
      "Epoch 3, Batch 60, Loss: 0.9818\n",
      "Epoch 3, Batch 70, Loss: 1.5180\n",
      "Epoch 3, Batch 80, Loss: 0.9699\n",
      "Epoch 3, Batch 90, Loss: 1.0678\n",
      "Epoch 3, Batch 100, Loss: 0.9633\n",
      "Epoch 3, Batch 110, Loss: 0.9591\n",
      "Epoch 3, Batch 120, Loss: 1.3955\n",
      "Epoch 3, Batch 130, Loss: 0.9832\n",
      "Epoch 3, Batch 140, Loss: 1.0861\n",
      "Epoch 3, Batch 150, Loss: 1.5788\n",
      "Epoch 3, Batch 160, Loss: 1.0235\n",
      "Epoch 3, Batch 170, Loss: 0.7363\n",
      "Epoch 3, Batch 180, Loss: 1.2416\n",
      "Epoch 4, Batch 0, Loss: 1.1985\n",
      "Epoch 4, Batch 10, Loss: 1.0561\n",
      "Epoch 4, Batch 20, Loss: 1.0378\n",
      "Epoch 4, Batch 30, Loss: 0.9143\n",
      "Epoch 4, Batch 40, Loss: 1.1616\n",
      "Epoch 4, Batch 50, Loss: 0.8360\n",
      "Epoch 4, Batch 60, Loss: 0.7780\n",
      "Epoch 4, Batch 70, Loss: 1.0201\n",
      "Epoch 4, Batch 80, Loss: 0.8920\n",
      "Epoch 4, Batch 90, Loss: 1.1630\n",
      "Epoch 4, Batch 100, Loss: 1.5763\n",
      "Epoch 4, Batch 110, Loss: 1.0806\n",
      "Epoch 4, Batch 120, Loss: 0.8369\n",
      "Epoch 4, Batch 130, Loss: 0.8342\n",
      "Epoch 4, Batch 140, Loss: 0.6009\n",
      "Epoch 4, Batch 150, Loss: 0.7346\n",
      "Epoch 4, Batch 160, Loss: 1.5037\n",
      "Epoch 4, Batch 170, Loss: 1.0108\n",
      "Epoch 4, Batch 180, Loss: 0.8977\n",
      "Epoch 5, Batch 0, Loss: 0.8610\n",
      "Epoch 5, Batch 10, Loss: 1.2083\n",
      "Epoch 5, Batch 20, Loss: 1.0819\n",
      "Epoch 5, Batch 30, Loss: 0.8924\n",
      "Epoch 5, Batch 40, Loss: 0.7813\n",
      "Epoch 5, Batch 50, Loss: 0.8640\n",
      "Epoch 5, Batch 60, Loss: 1.0617\n",
      "Epoch 5, Batch 70, Loss: 1.0518\n",
      "Epoch 5, Batch 80, Loss: 1.3784\n",
      "Epoch 5, Batch 90, Loss: 1.2008\n",
      "Epoch 5, Batch 100, Loss: 0.8842\n",
      "Epoch 5, Batch 110, Loss: 1.1372\n",
      "Epoch 5, Batch 120, Loss: 0.9144\n",
      "Epoch 5, Batch 130, Loss: 0.8846\n",
      "Epoch 5, Batch 140, Loss: 0.8173\n",
      "Epoch 5, Batch 150, Loss: 0.7420\n",
      "Epoch 5, Batch 160, Loss: 0.8838\n",
      "Epoch 5, Batch 170, Loss: 0.8772\n",
      "Epoch 5, Batch 180, Loss: 1.3431\n",
      "Epoch 6, Batch 0, Loss: 0.9431\n",
      "Epoch 6, Batch 10, Loss: 0.7855\n",
      "Epoch 6, Batch 20, Loss: 1.0867\n",
      "Epoch 6, Batch 30, Loss: 0.7514\n",
      "Epoch 6, Batch 40, Loss: 0.6463\n",
      "Epoch 6, Batch 50, Loss: 1.3084\n",
      "Epoch 6, Batch 60, Loss: 0.8852\n",
      "Epoch 6, Batch 70, Loss: 0.8939\n",
      "Epoch 6, Batch 80, Loss: 1.0661\n",
      "Epoch 6, Batch 90, Loss: 0.6245\n",
      "Epoch 6, Batch 100, Loss: 0.8149\n",
      "Epoch 6, Batch 110, Loss: 0.9964\n",
      "Epoch 6, Batch 120, Loss: 1.0247\n",
      "Epoch 6, Batch 130, Loss: 1.0335\n",
      "Epoch 6, Batch 140, Loss: 1.1432\n",
      "Epoch 6, Batch 150, Loss: 1.0602\n",
      "Epoch 6, Batch 160, Loss: 0.9724\n",
      "Epoch 6, Batch 170, Loss: 0.9490\n",
      "Epoch 6, Batch 180, Loss: 0.9538\n",
      "Epoch 7, Batch 0, Loss: 0.8109\n",
      "Epoch 7, Batch 10, Loss: 1.0874\n",
      "Epoch 7, Batch 20, Loss: 0.8326\n",
      "Epoch 7, Batch 30, Loss: 0.9040\n",
      "Epoch 7, Batch 40, Loss: 0.9945\n",
      "Epoch 7, Batch 50, Loss: 0.8147\n",
      "Epoch 7, Batch 60, Loss: 0.7029\n",
      "Epoch 7, Batch 70, Loss: 0.7428\n",
      "Epoch 7, Batch 80, Loss: 0.5661\n",
      "Epoch 7, Batch 90, Loss: 1.0864\n",
      "Epoch 7, Batch 100, Loss: 0.8684\n",
      "Epoch 7, Batch 110, Loss: 0.8448\n",
      "Epoch 7, Batch 120, Loss: 0.6848\n",
      "Epoch 7, Batch 130, Loss: 1.1795\n",
      "Epoch 7, Batch 140, Loss: 0.8183\n",
      "Epoch 7, Batch 150, Loss: 0.8970\n",
      "Epoch 7, Batch 160, Loss: 1.3624\n",
      "Epoch 7, Batch 170, Loss: 0.8720\n",
      "Epoch 7, Batch 180, Loss: 0.9179\n",
      "Epoch 8, Batch 0, Loss: 0.8402\n",
      "Epoch 8, Batch 10, Loss: 0.8870\n",
      "Epoch 8, Batch 20, Loss: 0.6605\n",
      "Epoch 8, Batch 30, Loss: 0.8653\n",
      "Epoch 8, Batch 40, Loss: 0.7959\n",
      "Epoch 8, Batch 50, Loss: 0.7052\n",
      "Epoch 8, Batch 60, Loss: 0.7752\n",
      "Epoch 8, Batch 70, Loss: 0.7879\n",
      "Epoch 8, Batch 80, Loss: 1.4648\n",
      "Epoch 8, Batch 90, Loss: 0.7156\n",
      "Epoch 8, Batch 100, Loss: 0.8052\n",
      "Epoch 8, Batch 110, Loss: 0.8492\n",
      "Epoch 8, Batch 120, Loss: 0.9952\n",
      "Epoch 8, Batch 130, Loss: 1.0315\n",
      "Epoch 8, Batch 140, Loss: 0.9681\n",
      "Epoch 8, Batch 150, Loss: 0.9740\n",
      "Epoch 8, Batch 160, Loss: 0.9896\n",
      "Epoch 8, Batch 170, Loss: 0.7386\n",
      "Epoch 8, Batch 180, Loss: 0.7736\n",
      "Epoch 9, Batch 0, Loss: 0.7850\n",
      "Epoch 9, Batch 10, Loss: 1.0768\n",
      "Epoch 9, Batch 20, Loss: 0.8779\n",
      "Epoch 9, Batch 30, Loss: 0.7590\n",
      "Epoch 9, Batch 40, Loss: 1.0323\n",
      "Epoch 9, Batch 50, Loss: 0.7827\n",
      "Epoch 9, Batch 60, Loss: 0.9154\n",
      "Epoch 9, Batch 70, Loss: 1.1440\n",
      "Epoch 9, Batch 80, Loss: 0.8352\n",
      "Epoch 9, Batch 90, Loss: 0.8518\n",
      "Epoch 9, Batch 100, Loss: 1.3635\n",
      "Epoch 9, Batch 110, Loss: 0.9455\n",
      "Epoch 9, Batch 120, Loss: 0.9923\n",
      "Epoch 9, Batch 130, Loss: 1.0110\n",
      "Epoch 9, Batch 140, Loss: 1.0018\n",
      "Epoch 9, Batch 150, Loss: 0.6713\n",
      "Epoch 9, Batch 160, Loss: 0.8699\n",
      "Epoch 9, Batch 170, Loss: 1.0812\n",
      "Epoch 9, Batch 180, Loss: 0.9701\n",
      "Epoch 10, Batch 0, Loss: 0.8408\n",
      "Epoch 10, Batch 10, Loss: 1.0083\n",
      "Epoch 10, Batch 20, Loss: 0.9969\n",
      "Epoch 10, Batch 30, Loss: 0.6679\n",
      "Epoch 10, Batch 40, Loss: 0.6791\n",
      "Epoch 10, Batch 50, Loss: 1.3899\n",
      "Epoch 10, Batch 60, Loss: 0.9772\n",
      "Epoch 10, Batch 70, Loss: 0.8338\n",
      "Epoch 10, Batch 80, Loss: 0.7843\n",
      "Epoch 10, Batch 90, Loss: 0.8514\n",
      "Epoch 10, Batch 100, Loss: 0.5743\n",
      "Epoch 10, Batch 110, Loss: 0.7510\n",
      "Epoch 10, Batch 120, Loss: 0.6038\n",
      "Epoch 10, Batch 130, Loss: 0.7134\n",
      "Epoch 10, Batch 140, Loss: 0.9948\n",
      "Epoch 10, Batch 150, Loss: 0.6540\n",
      "Epoch 10, Batch 160, Loss: 0.6416\n",
      "Epoch 10, Batch 170, Loss: 0.9163\n",
      "Epoch 10, Batch 180, Loss: 0.6372\n",
      "Epoch 11, Batch 0, Loss: 1.2431\n",
      "Epoch 11, Batch 10, Loss: 0.9769\n",
      "Epoch 11, Batch 20, Loss: 1.0419\n",
      "Epoch 11, Batch 30, Loss: 0.7841\n",
      "Epoch 11, Batch 40, Loss: 0.5683\n",
      "Epoch 11, Batch 50, Loss: 1.1505\n",
      "Epoch 11, Batch 60, Loss: 0.9917\n",
      "Epoch 11, Batch 70, Loss: 0.8270\n",
      "Epoch 11, Batch 80, Loss: 0.7618\n",
      "Epoch 11, Batch 90, Loss: 1.0851\n",
      "Epoch 11, Batch 100, Loss: 1.1679\n",
      "Epoch 11, Batch 110, Loss: 0.8483\n",
      "Epoch 11, Batch 120, Loss: 0.8221\n",
      "Epoch 11, Batch 130, Loss: 0.7383\n",
      "Epoch 11, Batch 140, Loss: 0.7168\n",
      "Epoch 11, Batch 150, Loss: 0.8872\n",
      "Epoch 11, Batch 160, Loss: 0.8805\n",
      "Epoch 11, Batch 170, Loss: 0.8124\n",
      "Epoch 11, Batch 180, Loss: 0.6796\n",
      "Epoch 12, Batch 0, Loss: 0.8069\n",
      "Epoch 12, Batch 10, Loss: 0.8166\n",
      "Epoch 12, Batch 20, Loss: 0.9370\n",
      "Epoch 12, Batch 30, Loss: 0.8737\n",
      "Epoch 12, Batch 40, Loss: 0.9472\n",
      "Epoch 12, Batch 50, Loss: 0.7974\n",
      "Epoch 12, Batch 60, Loss: 1.1392\n",
      "Epoch 12, Batch 70, Loss: 0.7287\n",
      "Epoch 12, Batch 80, Loss: 0.9047\n",
      "Epoch 12, Batch 90, Loss: 0.9295\n",
      "Epoch 12, Batch 100, Loss: 0.8845\n",
      "Epoch 12, Batch 110, Loss: 0.9306\n",
      "Epoch 12, Batch 120, Loss: 0.7890\n",
      "Epoch 12, Batch 130, Loss: 1.2206\n",
      "Epoch 12, Batch 140, Loss: 1.1130\n",
      "Epoch 12, Batch 150, Loss: 0.7651\n",
      "Epoch 12, Batch 160, Loss: 0.8614\n",
      "Epoch 12, Batch 170, Loss: 0.7645\n",
      "Epoch 12, Batch 180, Loss: 0.8282\n",
      "Epoch 13, Batch 0, Loss: 0.8885\n",
      "Epoch 13, Batch 10, Loss: 0.6356\n",
      "Epoch 13, Batch 20, Loss: 1.1718\n",
      "Epoch 13, Batch 30, Loss: 0.9554\n",
      "Epoch 13, Batch 40, Loss: 0.6041\n",
      "Epoch 13, Batch 50, Loss: 0.9388\n",
      "Epoch 13, Batch 60, Loss: 1.0683\n",
      "Epoch 13, Batch 70, Loss: 0.8822\n",
      "Epoch 13, Batch 80, Loss: 0.9740\n",
      "Epoch 13, Batch 90, Loss: 0.6438\n",
      "Epoch 13, Batch 100, Loss: 1.1361\n",
      "Epoch 13, Batch 110, Loss: 0.9655\n",
      "Epoch 13, Batch 120, Loss: 0.9099\n",
      "Epoch 13, Batch 130, Loss: 0.8424\n",
      "Epoch 13, Batch 140, Loss: 0.6824\n",
      "Epoch 13, Batch 150, Loss: 0.7048\n",
      "Epoch 13, Batch 160, Loss: 0.9910\n",
      "Epoch 13, Batch 170, Loss: 0.6936\n",
      "Epoch 13, Batch 180, Loss: 0.5359\n",
      "Epoch 14, Batch 0, Loss: 0.5818\n",
      "Epoch 14, Batch 10, Loss: 0.8900\n",
      "Epoch 14, Batch 20, Loss: 0.6335\n",
      "Epoch 14, Batch 30, Loss: 0.7998\n",
      "Epoch 14, Batch 40, Loss: 0.9638\n",
      "Epoch 14, Batch 50, Loss: 0.7613\n",
      "Epoch 14, Batch 60, Loss: 1.0702\n",
      "Epoch 14, Batch 70, Loss: 0.8473\n",
      "Epoch 14, Batch 80, Loss: 0.6793\n",
      "Epoch 14, Batch 90, Loss: 0.7058\n",
      "Epoch 14, Batch 100, Loss: 1.0381\n",
      "Epoch 14, Batch 110, Loss: 1.3893\n",
      "Epoch 14, Batch 120, Loss: 1.0485\n",
      "Epoch 14, Batch 130, Loss: 0.7786\n",
      "Epoch 14, Batch 140, Loss: 0.8111\n",
      "Epoch 14, Batch 150, Loss: 0.6489\n",
      "Epoch 14, Batch 160, Loss: 0.8964\n",
      "Epoch 14, Batch 170, Loss: 0.9707\n",
      "Epoch 14, Batch 180, Loss: 0.8596\n",
      "Epoch 15, Batch 0, Loss: 0.7441\n",
      "Epoch 15, Batch 10, Loss: 1.0337\n",
      "Epoch 15, Batch 20, Loss: 0.6956\n",
      "Epoch 15, Batch 30, Loss: 0.8237\n",
      "Epoch 15, Batch 40, Loss: 0.6234\n",
      "Epoch 15, Batch 50, Loss: 0.7086\n",
      "Epoch 15, Batch 60, Loss: 0.9839\n",
      "Epoch 15, Batch 70, Loss: 1.0668\n",
      "Epoch 15, Batch 80, Loss: 0.7184\n",
      "Epoch 15, Batch 90, Loss: 1.2147\n",
      "Epoch 15, Batch 100, Loss: 0.6968\n",
      "Epoch 15, Batch 110, Loss: 0.8998\n",
      "Epoch 15, Batch 120, Loss: 0.6758\n",
      "Epoch 15, Batch 130, Loss: 0.7326\n",
      "Epoch 15, Batch 140, Loss: 1.1127\n",
      "Epoch 15, Batch 150, Loss: 0.7859\n",
      "Epoch 15, Batch 160, Loss: 0.8643\n",
      "Epoch 15, Batch 170, Loss: 0.7094\n",
      "Epoch 15, Batch 180, Loss: 0.6900\n",
      "Epoch 16, Batch 0, Loss: 1.2386\n",
      "Epoch 16, Batch 10, Loss: 0.9372\n",
      "Epoch 16, Batch 20, Loss: 0.7691\n",
      "Epoch 16, Batch 30, Loss: 0.8696\n",
      "Epoch 16, Batch 40, Loss: 0.8593\n",
      "Epoch 16, Batch 50, Loss: 1.0912\n",
      "Epoch 16, Batch 60, Loss: 0.6381\n",
      "Epoch 16, Batch 70, Loss: 1.0378\n",
      "Epoch 16, Batch 80, Loss: 1.2105\n",
      "Epoch 16, Batch 90, Loss: 0.7723\n",
      "Epoch 16, Batch 100, Loss: 0.6749\n",
      "Epoch 16, Batch 110, Loss: 0.7434\n",
      "Epoch 16, Batch 120, Loss: 0.7866\n",
      "Epoch 16, Batch 130, Loss: 0.7826\n",
      "Epoch 16, Batch 140, Loss: 0.9818\n",
      "Epoch 16, Batch 150, Loss: 1.0807\n",
      "Epoch 16, Batch 160, Loss: 0.8519\n",
      "Epoch 16, Batch 170, Loss: 0.8333\n",
      "Epoch 16, Batch 180, Loss: 1.1031\n",
      "Epoch 17, Batch 0, Loss: 1.0358\n",
      "Epoch 17, Batch 10, Loss: 0.7737\n",
      "Epoch 17, Batch 20, Loss: 1.1094\n",
      "Epoch 17, Batch 30, Loss: 0.8256\n",
      "Epoch 17, Batch 40, Loss: 0.9197\n",
      "Epoch 17, Batch 50, Loss: 0.9957\n",
      "Epoch 17, Batch 60, Loss: 0.8649\n",
      "Epoch 17, Batch 70, Loss: 1.4897\n",
      "Epoch 17, Batch 80, Loss: 1.0117\n",
      "Epoch 17, Batch 90, Loss: 0.9824\n",
      "Epoch 17, Batch 100, Loss: 0.7818\n",
      "Epoch 17, Batch 110, Loss: 0.6795\n",
      "Epoch 17, Batch 120, Loss: 0.7538\n",
      "Epoch 17, Batch 130, Loss: 0.6953\n",
      "Epoch 17, Batch 140, Loss: 0.6599\n",
      "Epoch 17, Batch 150, Loss: 1.1034\n",
      "Epoch 17, Batch 160, Loss: 0.5945\n",
      "Epoch 17, Batch 170, Loss: 0.9976\n",
      "Epoch 17, Batch 180, Loss: 0.7006\n",
      "Epoch 18, Batch 0, Loss: 0.7540\n",
      "Epoch 18, Batch 10, Loss: 0.7452\n",
      "Epoch 18, Batch 20, Loss: 0.6986\n",
      "Epoch 18, Batch 30, Loss: 1.0870\n",
      "Epoch 18, Batch 40, Loss: 0.8479\n",
      "Epoch 18, Batch 50, Loss: 0.7924\n",
      "Epoch 18, Batch 60, Loss: 0.7034\n",
      "Epoch 18, Batch 70, Loss: 0.7969\n",
      "Epoch 18, Batch 80, Loss: 0.8014\n",
      "Epoch 18, Batch 90, Loss: 0.6542\n",
      "Epoch 18, Batch 100, Loss: 0.9043\n",
      "Epoch 18, Batch 110, Loss: 0.7766\n",
      "Epoch 18, Batch 120, Loss: 0.6630\n",
      "Epoch 18, Batch 130, Loss: 0.6270\n",
      "Epoch 18, Batch 140, Loss: 1.1042\n",
      "Epoch 18, Batch 150, Loss: 0.9626\n",
      "Epoch 18, Batch 160, Loss: 0.7242\n",
      "Epoch 18, Batch 170, Loss: 0.9637\n",
      "Epoch 18, Batch 180, Loss: 0.6356\n",
      "Epoch 19, Batch 0, Loss: 0.8039\n",
      "Epoch 19, Batch 10, Loss: 0.8232\n",
      "Epoch 19, Batch 20, Loss: 1.0254\n",
      "Epoch 19, Batch 30, Loss: 0.7314\n",
      "Epoch 19, Batch 40, Loss: 0.9837\n",
      "Epoch 19, Batch 50, Loss: 0.6915\n",
      "Epoch 19, Batch 60, Loss: 0.8971\n",
      "Epoch 19, Batch 70, Loss: 1.0952\n",
      "Epoch 19, Batch 80, Loss: 0.6065\n",
      "Epoch 19, Batch 90, Loss: 0.7640\n",
      "Epoch 19, Batch 100, Loss: 0.9532\n",
      "Epoch 19, Batch 110, Loss: 1.0643\n",
      "Epoch 19, Batch 120, Loss: 1.1098\n",
      "Epoch 19, Batch 130, Loss: 0.8340\n",
      "Epoch 19, Batch 140, Loss: 0.9469\n",
      "Epoch 19, Batch 150, Loss: 0.7506\n",
      "Epoch 19, Batch 160, Loss: 0.8200\n",
      "Epoch 19, Batch 170, Loss: 0.7784\n",
      "Epoch 19, Batch 180, Loss: 1.1429\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428b4020-92a8-4116-bb8f-3b1933e80558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 76.53%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c06540-1283-4c29-a346-8b2c8fc89221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  # your trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084f53e4-5da4-4cb9-b84b-8fe2e824fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e24ee971-2951-4152-af98-7ef65ea54a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trained model saved as 'models/best_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), 'models/best_model.pth')\n",
    "print(\"✅ Trained model saved as 'models/best_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "343d4e68-8e71-4eb2-8af0-e0ba46f0111a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet-pytorch in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from efficientnet-pytorch) (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet-pytorch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet-pytorch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet-pytorch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet-pytorch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet-pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet-pytorch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch->efficientnet-pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->efficientnet-pytorch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9582985-08dc-41aa-9ac4-960a4ce33f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to C:\\Users\\namra/.cache\\torch\\hub\\checkpoints\\efficientnet-b0-355c32eb.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 20.4M/20.4M [00:19<00:00, 1.13MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Use EfficientNet‑B0 for faster training on CPU\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=5)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80844ec1-2e70-403f-af8e-13477d310a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((260, 260)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Crop to 224x224 for faster training\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15, fill=0),\n",
    "\n",
    "    # Added small brightness/contrast variation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c15a245a-62d8-4e05-b08d-e05d9cf11099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\namra\\appdata\\roaming\\python\\python311\\site-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (1.15.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\users\\namra\\appdata\\roaming\\python\\python311\\site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\namra\\appdata\\roaming\\python\\python311\\site-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore==0.0.24->albumentations) (6.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: albumentations[torch] in c:\\users\\namra\\appdata\\roaming\\python\\python311\\site-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations[torch]) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations[torch]) (1.15.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations[torch]) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations[torch]) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\users\\namra\\appdata\\roaming\\python\\python311\\site-packages (from albumentations[torch]) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\namra\\appdata\\roaming\\python\\python311\\site-packages (from albumentations[torch]) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore==0.0.24->albumentations[torch]) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore==0.0.24->albumentations[torch]) (6.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations[torch]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations[torch]) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations[torch]) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations[torch]) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: albumentations 2.0.8 does not provide the extra 'torch'\n"
     ]
    }
   ],
   "source": [
    "%pip install albumentations\n",
    "%pip install albumentations[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c6d6fe-363b-45b7-b3c5-3818253121bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Using cached albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (1.15.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (2.11.7)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Using cached albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore==0.0.24->albumentations) (6.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\namra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
      "Using cached albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Using cached albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "Installing collected packages: opencv-python-headless, albucore, albumentations\n",
      "\n",
      "   ---------------------------------------- 0/3 [opencv-python-headless]\n",
      "   ---------------------------------------- 0/3 [opencv-python-headless]\n",
      "   ---------------------------------------- 0/3 [opencv-python-headless]\n",
      "   ------------- -------------------------- 1/3 [albucore]\n",
      "   -------------------------- ------------- 2/3 [albumentations]\n",
      "   ---------------------------------------- 3/3 [albumentations]\n",
      "\n",
      "Successfully installed albucore-0.0.24 albumentations-2.0.8 opencv-python-headless-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install albumentations --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3487be64-70f1-4b36-a24c-bb0ad2b78a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mA\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[32m      5\u001b[39m cv2.setNumThreads(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.3),\n",
    "    A.RandomResizedCrop(224, 224, scale=(0.8,1.0), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, p=0.5),\n",
    "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa43872-d646-41b4-bc4b-0ebd1c78870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class_weights = compute_class_weight('balanced',\n",
    "                                     classes=np.unique(train_df['diagnosis']),\n",
    "                                     y=train_df['diagnosis'])\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1832eb9-ef91-418a-b4a4-e8548f11545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72203527-4976-4695-bced-747631a53577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa86693b-c9e4-4417-a493-4fd9b99ecae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['filepath'] = train_df['id_code'].apply(lambda x: f\"data/train_images/{x}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69bae5f9-a38e-457c-9dcc-17bb4533e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df['filepath'] = test_df['id_code'].apply(lambda x: f\"data/test_images/{x}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "403398ca-8f97-4d4f-84b9-b47f24604d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "missing = train_df.loc[~train_df['filepath'].apply(os.path.exists), 'filepath']\n",
    "print(\"Missing files:\", len(missing))\n",
    "if not missing.empty:\n",
    "    print(missing.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721ee7e7-7c68-47f8-8e6d-50d2e5083ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def clahe_pil(img):\n",
    "    img = np.array(img.convert('RGB'))[...,::-1]  # to OpenCV BGR\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    lab[...,0] = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(lab[...,0])\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    return Image.fromarray(img[...,::-1])        # back to RGB PIL\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Lambda(clahe_pil),\n",
    "    transforms.Resize((260, 260)),\n",
    "    transforms.RandomResizedCrop(260, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "718abe4f-e149-4bcb-b764-129b5d71faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        img = Image.open(row['filepath']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "# Split 80/20 for training + validation\n",
    "train_size = int(0.8 * len(train_df))\n",
    "val_size   = len(train_df) - train_size\n",
    "train_df_subset, val_df_subset = random_split(train_df, [train_size, val_size],\n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Build datasets\n",
    "train_ds = RetinopathyDataset(train_df.loc[train_df_subset.indices], transform=data_transforms)\n",
    "val_ds   = RetinopathyDataset(train_df.loc[val_df_subset.indices], transform=data_transforms)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ea1fff5-56cd-4cf5-9135-eb81c3cb38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['diagnosis']),\n",
    "    y=train_df['diagnosis']\n",
    ")\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce5da9-737a-498f-9c6f-275e15c0ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_val = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ⬅️ This line should be aligned here (no extra spaces before 'if')\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                  f\"Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                  f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, preds = torch.max(model(images), 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save(model.state_dict(), 'models/best_model.pth')\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c88bf-995f-4575-ac22-244aeecd4e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
